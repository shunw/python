{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.6.5-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python36564bitff28b0bc308b43728a982148efad7fb6",
   "display_name": "Python 3.6.5 64-bit"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weibull Distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is no distribution called weibull in scipy. There are \n",
    "\n",
    "- weibull_min\n",
    "    \n",
    "- weibull_max \n",
    "    \n",
    "- exponweib\n",
    "    \n",
    "weibull_min is the one that matches the wikipedia article on the Weibull distribuition. weibull_min has three parameters: c (shape), loc (location) and scale (scale). c and scale correspond to k and Î» in the wikipedia article, respectively. (They aren't shown in the formula in the docstring, but all the scipy distributions have loc and scale parameters.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "2.03891122445 579314.463827\n0.904894886393\n"
    }
   ],
   "source": [
    "from scipy.stats import weibull_min\n",
    "import numpy as np\n",
    "\n",
    "data = np.array([10, 30, 50, 60])\n",
    "data = np.array([.1591, .3864, .6136, .8409])\n",
    "data = np.array([16, 34, 53, 75, 93, 120])\n",
    "fail_times = [\n",
    "    400126, 150560, 950830, 526743, \n",
    "    916478, 680649, 471434, 679522, \n",
    "    776811, 400007, 150280, 150278, \n",
    "    412765\n",
    "]\n",
    "data = np.array(fail_times)\n",
    "shape, loc, scale = weibull_min.fit(data, floc = 0)\n",
    "print (shape, scale)\n",
    "\n",
    "life = 300000\n",
    "p = weibull_min.cdf(life, 3.5, loc, scale)\n",
    "print (1 - p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the Sample Size\n",
    "\n",
    "[reference](https://zhuanlan.zhihu.com/p/33752114)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple-Censored Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": "WARNING:weibull.weibull:the linear regression method is likely to yield better results with 13 data points\nfit method          maximum likelihood estimation\nconfidence                                    0.9\nbeta lower limit                          5.22517\nbeta nominal                              13.3184\nbeta upper limit                          33.9472\neta lower limit                            860061\neta nominal                                939458\neta upper limit                       1.02618e+06\nmean life                                  903647\nmedian life                                913958\nb10 life                                   793407\ndtype: object\n"
    }
   ],
   "source": [
    "import weibull\n",
    "%matplotlib inline\n",
    "\n",
    "fail_times = [\n",
    "    400126, 150560, 950830, 526743, \n",
    "    916478, 680649, 471434, 679522, \n",
    "    776811, 400007, 150280, 150278, \n",
    "    412765\n",
    "]\n",
    "\n",
    "suspensions = [1, 1, 0, 1,\n",
    "               1, 1, 1, 1,\n",
    "               0, 1, 1, 1,\n",
    "               1\n",
    "               ]\n",
    "\n",
    "# this is where the actual analysis and curve fitting occur\n",
    "analysis = weibull.Analysis(fail_times, suspensions, unit='hour')\n",
    "analysis.fit(method='mle', confidence_level=0.9)\n",
    "\n",
    "print(analysis.stats)\n",
    "\n",
    "# analysis.probplot(file_name='gallery-probplot.png')\n",
    "\n",
    "# analysis.pdf(file_name='gallery-pdf.png')\n",
    "# analysis.hazard(file_name='gallery-hazard.png')\n",
    "# analysis.sf(file_name='gallery-survival.png')\n",
    "# analysis.fr(file_name='gallery-fr.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exponential Distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Sample Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "285000.0\n46.0517018599 2.18745583834\n"
    }
   ],
   "source": [
    "import scipy.stats as st\n",
    "\n",
    "def sample_size_exp(alpha, lower_limit, occur_qty): \n",
    "    '''\n",
    "    lower_limit: \n",
    "        1. MIBF spec; \n",
    "        2. 1/ (1 - reliability) => 1/ failure rate\n",
    "    '''\n",
    "    lower = st.chi2.ppf(1 - alpha, 2 * (occur_qty + 1))\n",
    "    return lower * lower_limit / 2\n",
    "\n",
    "base_life = 300000\n",
    "\n",
    "rel = .95\n",
    "lower_limit =  1/ (1 - rel)\n",
    "# print (lower_limit)\n",
    "\n",
    "n_a = sample_size_exp(.1, lower_limit, 0) \n",
    "\n",
    "print (base_life * rel)\n",
    "image_b = sample_size_exp(.1, base_life * rel, 0) / base_life\n",
    "\n",
    "print (n_a, image_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Lower Upper Limit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "6666483\n317394.851467 18537120.9113\n"
    }
   ],
   "source": [
    "def exp_conf_interval(alpha, page_number, jam_occur): \n",
    "    '''\n",
    "    this is to calculate the confidence interval for exponential distribution\n",
    "    '''\n",
    "    upper = 2 * page_number / st.chi2.ppf(alpha/2, 2 * jam_occur)\n",
    "    lower = 2 * page_number / st.chi2.ppf((1 - alpha/2), 2 * jam_occur)\n",
    "    return (lower, upper)\n",
    "\n",
    "fail_times = np.array([\n",
    "    400126, 150560, 950830, 526743, \n",
    "    916478, 680649, 471434, 679522, \n",
    "    776811, 400007, 150280, 150278, \n",
    "    412765\n",
    "])\n",
    "page_number = fail_times.sum()\n",
    "print (page_number)\n",
    "lower, upper = exp_conf_interval(.1, fail_times.max(), 1)\n",
    "print (lower, upper)"
   ]
  }
 ]
}